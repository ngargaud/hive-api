{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e27a16-1661-44d2-904f-930ccd5d6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --force-reinstall hive_api-0.0.4-py3-none-any.whl\n",
    "!{sys.executable} -m pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0e3a6-3e47-4f59-afcd-e0ccea90789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hive_api import HiveApi\n",
    "hive = HiveApi()\n",
    "\n",
    "from hive_agent import HiveAgent, AgentState\n",
    "agent = HiveAgent(hive, debug_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ff87a-1846-4f5b-be53-0072295b43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "class ConversationGraph():\n",
    "    def __init__(self, ollama_url=\"http://ollama:11434\", ollama_model=\"mistral:latest\"):\n",
    "        self._node_name = \"conversation_node\"\n",
    "        self._json_model = OllamaLLM(base_url=ollama_url, model=ollama_model, temperature=0.7)\n",
    "        self._prompt = PromptTemplate.from_template(\"\"\"You are a smart assistant.\n",
    "Answer if you know else tell the user you can not provide any information and describe how to find the answer.\n",
    "\n",
    "{conversation}\n",
    "\n",
    "\"\"\")\n",
    "        self._chain = self._prompt | self._json_model\n",
    "\n",
    "    def node_name(self):\n",
    "        return self._node_name\n",
    "\n",
    "    def chain(self):\n",
    "        return self._chain\n",
    "\n",
    "    def node(self, state: AgentState, config):\n",
    "        \"\"\" Answer the user message \"\"\"\n",
    "        print(\"{}.node() called\".format( self._node_name))\n",
    "        state[\"messages\"] = AIMessage(content=self._chain.invoke({\"conversation\": state[\"messages\"]}))\n",
    "        return state\n",
    "    \n",
    "    def edge(self, state: AgentState, config):\n",
    "        \"\"\" Nothing special here \"\"\"\n",
    "        return state\n",
    "\n",
    "\n",
    "class SubGraph():\n",
    "    def __init__(self, hive):\n",
    "        self.name = \"SubGraph.workflow()\"\n",
    "        self.conversation = ConversationGraph(ollama_url=hive.get_api_url(\"ollama\"))\n",
    "        self.conversation2 = ConversationGraph(ollama_url=hive.get_api_url(\"ollama\"))\n",
    "\n",
    "    def debug_node(self, state: AgentState, config):\n",
    "        user = config[\"configurable\"].get(\"user\", \"unknown\")\n",
    "        print(\"SubGraph.debug_node() User {} request\".format(user))\n",
    "        return state\n",
    "    \n",
    "    def workflow(self, subgraph=None, subname=\"debug_node\", memory=None):\n",
    "        # Define a new graph\n",
    "        workflow = StateGraph(AgentState)\n",
    "        subnode = self.debug_node\n",
    "        if subgraph:\n",
    "            subnode = subgraph\n",
    "            if subname == \"debug_node\":\n",
    "                subname = \"subgraph\"\n",
    "\n",
    "        # Define the two nodes we will cycle between\n",
    "        workflow.add_node(\"SubGraph.conversation\", self.conversation.node)\n",
    "        workflow.add_node(\"SubGraph.conversation2\", self.conversation2.node)\n",
    "        workflow.add_node(\"SubGraph.{}\".format(subname), subnode)\n",
    "        workflow.set_entry_point(\"SubGraph.conversation\")\n",
    "\n",
    "        workflow.add_edge(\"SubGraph.conversation\", \"SubGraph.{}\".format(subname))\n",
    "        workflow.add_edge(\"SubGraph.{}\".format(subname), \"SubGraph.conversation2\")\n",
    "        workflow.add_edge(\"SubGraph.conversation2\", END)\n",
    "        if memory:\n",
    "            return workflow.compile(checkpointer=memory)\n",
    "        else:\n",
    "            return workflow.compile()\n",
    "\n",
    "\n",
    "subgraph = SubGraph(hive)\n",
    "agent.build_graph(subgraph.workflow(memory=agent.get_memory()))\n",
    "agent.show_graph(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62dda4-47c1-4065-aced-67161bb3e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in agent.on_input(\"bonjour\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa8a25-79d3-4610-83e0-b74e0f9f6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in agent.on_input(\"alors Ã§a donne quoi\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4274b5-306e-4ae5-ab9d-7d6ae1110d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
